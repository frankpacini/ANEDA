{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "defd8e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import random\n",
    "\n",
    "def hide_toggle(for_next=False):\n",
    "    this_cell = \"\"\"$('div.cell.code_cell.rendered.selected')\"\"\"\n",
    "    next_cell = this_cell + '.next()'\n",
    "\n",
    "    toggle_text = 'Toggle show/hide'  # text shown on toggle link\n",
    "    target_cell = this_cell  # target cell to control with toggle\n",
    "    js_hide_current = ''  # bit of JS to permanently hide code in current cell (only when toggling next cell)\n",
    "\n",
    "    if for_next:\n",
    "        target_cell = next_cell\n",
    "        toggle_text += ' next cell'\n",
    "        js_hide_current = this_cell + '.find(\"div.input\").hide();'\n",
    "\n",
    "    js_f_name = 'code_toggle_{}'.format(str(random.randint(1,2**64)))\n",
    "\n",
    "    html = \"\"\"\n",
    "        <script>\n",
    "            function {f_name}() {{\n",
    "                {cell_selector}.find('div.input').toggle();\n",
    "            }}\n",
    "\n",
    "            {js_hide_current}\n",
    "        </script>\n",
    "\n",
    "        <a href=\"javascript:{f_name}()\">{toggle_text}</a>\n",
    "    \"\"\".format(\n",
    "        f_name=js_f_name,\n",
    "        cell_selector=target_cell,\n",
    "        js_hide_current=js_hide_current, \n",
    "        toggle_text=toggle_text\n",
    "    )\n",
    "\n",
    "    return HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d931ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c76c107f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_4489525508991869922() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_4489525508991869922()\">Toggle show/hide</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %load datasets_generator.py\n",
    "import os.path\n",
    "\n",
    "import dgl\n",
    "\n",
    "import data_helper\n",
    "import landmarks\n",
    "import node2vec\n",
    "from data_helper import read_file\n",
    "import utilities\n",
    "\n",
    "def create_train_val_test_sets(file_name, force_recreate_datasets, write_train_val_test,portion,method):\n",
    "    final_output_path = f\"../output/datasets/{file_name}_train_val_test.pkl\"\n",
    "\n",
    "    if not force_recreate_datasets and os.path.isfile(final_output_path):  ## if the file already exists\n",
    "        print(f\"datasets for '{file_name}' already exists, only read them back!\")\n",
    "        return read_file(final_output_path)\n",
    "\n",
    "    # TODO: should keep the config in one separate file\n",
    "    ##### Step 1. Read data\n",
    "    ## `file_name` is an edgelist file, no extension needed\n",
    "    # \"small_test\" is a small dataset for testing, default data should be \"socfb-American75\"\n",
    "    # file_name: {\"ego-facebook-original\", \"small_test\", \"socfb-American75\", ...}\n",
    "\n",
    "    ## Load input file into a DGL graph\n",
    "    input_path = f\"../data/{file_name}.edgelist\"\n",
    "    graph = data_helper.load_edgelist_file_to_dgl_graph(path=input_path, undirected=True,\n",
    "                                                        edge_weights=None)\n",
    "\n",
    "    #####  Step 2. Run Node2Vec to get the embedding\n",
    "    # Node2Vec params\n",
    "    node2vec_args = {\n",
    "        \"device\": \"cuda\",\n",
    "        \"embedding_dim\": 128,\n",
    "        \"walk_length\": 5,  # 80\n",
    "        \"window_size\": 5,  # 10\n",
    "        \"p\": 0.2,  # 0.25,\n",
    "        \"q\": 4,  # 4.0,\n",
    "        \"num_walks\": 25,\n",
    "        \"epochs\": 100,  # 100\n",
    "        \"batch_size\": 128,\n",
    "        \"learning_rate\": 0.01,\n",
    "    }\n",
    "    embedding_output_path = f\"../output/embedding/{file_name}_embed.pkl\"\n",
    "    if(os.path.isfile(embedding_output_path)):\n",
    "        embedding = read_file(embedding_output_path)\n",
    "    # took ~6mins/epoch to get Node2Vec for \"socfb-American75\", using 8GB RAM, 4CPU Macbook\n",
    "    else:\n",
    "        embedding = node2vec.run_node2vec(graph, eval_set=None, args=node2vec_args, output_path=embedding_output_path)\n",
    "    print(f\"Done embedding {file_name}!\")\n",
    "    nx_graph = dgl.to_networkx(graph)\n",
    "    if(method == 'random'):\n",
    "        ## Option 1: Get a few landmark nodes randomly from the graph:\n",
    "        random_seed = 2021\n",
    "        num_landmarks = int(len(nx_graph)*portion)\n",
    "        landmark_nodes = landmarks.get_landmark_nodes(num_landmarks, nx_graph, random_seed=random_seed)\n",
    "    else:\n",
    "        ##Get customize landmark nodes \n",
    "        landmark_nodes = landmarks.get_landmark_custom(nx_graph,portion = portion)\n",
    "        \n",
    "    #####  Step 3: Create labels:\n",
    "    # We convert the `dgl` graph to `networkx` graph. We will use networkx for finding the shortest path\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    ## Option 2: set `num_landmarks` to `graph.num_nodes()` to make all the nodes as landmark nodes.\n",
    "    ## TODO: when all nodes are landmark nodes, might need a better way to calc the distance (symmetric matrix)\n",
    "    # num_landmarks = nx_graph.number_of_nodes()\n",
    "    \n",
    "    \n",
    "    ## Get landmark nodes:\n",
    "   \n",
    "    \n",
    "   \n",
    "    \n",
    "    # Get landmarks' distance: get distance of every pair (l,n), where l is a landmark node, n is a node in the graph\n",
    "    landmark_distance_output = f\"../output/landmarks_distance/{file_name}_dist.pkl\"  # where to store the output file\n",
    "    print(\"Calculating landmarks distance...\")\n",
    "    distance_map = landmarks.calculate_landmarks_distance(landmark_nodes, nx_graph,\n",
    "                                                          output_path=landmark_distance_output)\n",
    "    print(\"Done landmarks distance!\")\n",
    "\n",
    "    ## Plot the network\n",
    "    ##utilities.plot_nx_graph(nx_graph, file_name=file_name)\n",
    "\n",
    "    ##### Step 4: Create datasets to train a model\n",
    "    print(\"creating datasets...\")\n",
    "    x, y = data_helper.create_dataset(distance_map, embedding)\n",
    "    x, y = data_helper.remove_data_with_a_few_observations(x, y)\n",
    "    test_size = 0.25\n",
    "    val_size = 0.15\n",
    "    train_val_test_path = \"../output/datasets\"\n",
    "    datasets = data_helper.train_valid_test_split(x, y, test_size=test_size, val_size=val_size,\n",
    "                                                  output_path=train_val_test_path,\n",
    "                                                  file_name=file_name, write_train_val_test=write_train_val_test,random_seed = 2021)\n",
    "    return datasets\n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3fd009cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score,mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2a0abed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_33 (Dense)            (None, 200)               25800     \n",
      "                                                                 \n",
      " batch_normalization_25 (Bat  (None, 200)              800       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_24 (ReLU)             (None, 200)               0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 100)               20100     \n",
      "                                                                 \n",
      " batch_normalization_26 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_25 (ReLU)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 50)                5050      \n",
      "                                                                 \n",
      " batch_normalization_27 (Bat  (None, 50)               200       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_26 (ReLU)             (None, 50)                0         \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 52,401\n",
      "Trainable params: 51,701\n",
      "Non-trainable params: 700\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def poisson_loss(y_true, y_pred):\n",
    "            \"\"\"\n",
    "            Custom loss function for Poisson model.\n",
    "            Equivalent Keras implementation for reference:\n",
    "            K.mean(y_pred - y_true * math_ops.log(y_pred + K.epsilon()), axis=-1)\n",
    "            For output of shape (2,3) it return (2,) vector. Need to calculate\n",
    "            mean of that too.\n",
    "            \"\"\"\n",
    "            y_pred = tf.squeeze(y_pred)\n",
    "            loss = tf.reduce_mean(y_pred - y_true * tf.math.log(y_pred+1e-7),axis=-1)\n",
    "#             y_pred = torch.squeeze(y_pred)\n",
    "#             loss = torch.mean(y_pred - y_true * torch.log(y_pred+1e-7))\n",
    "            return loss\n",
    "\n",
    "def cons_model():\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    \"\"\"    \n",
    "    \"\"\"\n",
    "    model.add(tf.keras.layers.Dense(200))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.ReLU())\n",
    "    model.add(tf.keras.layers.Dense(100))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.ReLU())\n",
    "    model.add(tf.keras.layers.Dense(50))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.ReLU())\n",
    "    model.add(tf.keras.layers.Dense(1,activation = 'softplus'))\n",
    "    \n",
    "#     model.add(tf.keras.layers.Conv2D(filters=1, kernel_size=(1, 1),activation='sigmoid',padding='same', data_format='channels_last')) \n",
    "    model.build((None,128))\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "my_model = cons_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "14af03b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.compile(loss=poisson_loss, optimizer=tf.keras.optimizers.RMSprop(\n",
    "    learning_rate=0.001, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False,name='RMSprop'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cb387f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "116/116 [==============================] - 1s 5ms/step - loss: -1.7712 - val_loss: -1.9698\n",
      "Epoch 2/100\n",
      "116/116 [==============================] - 1s 4ms/step - loss: -3.1596 - val_loss: -3.0367\n",
      "Epoch 3/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1872 - val_loss: -3.1624\n",
      "Epoch 4/100\n",
      "116/116 [==============================] - 1s 5ms/step - loss: -3.1885 - val_loss: -3.1826\n",
      "Epoch 5/100\n",
      "116/116 [==============================] - 1s 4ms/step - loss: -3.1891 - val_loss: -3.1863\n",
      "Epoch 6/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1893 - val_loss: -3.1790\n",
      "Epoch 7/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1896 - val_loss: -3.1880\n",
      "Epoch 8/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1897 - val_loss: -3.1861\n",
      "Epoch 9/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1898 - val_loss: -3.1870\n",
      "Epoch 10/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1898 - val_loss: -3.1888\n",
      "Epoch 11/100\n",
      "116/116 [==============================] - 1s 4ms/step - loss: -3.1899 - val_loss: -3.1894\n",
      "Epoch 12/100\n",
      "116/116 [==============================] - 1s 4ms/step - loss: -3.1900 - val_loss: -3.1895\n",
      "Epoch 13/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1900 - val_loss: -3.1890\n",
      "Epoch 14/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1900 - val_loss: -3.1868\n",
      "Epoch 15/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1901 - val_loss: -3.1889\n",
      "Epoch 16/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1900 - val_loss: -3.1895\n",
      "Epoch 17/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1901 - val_loss: -3.1881\n",
      "Epoch 18/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1902 - val_loss: -3.1884\n",
      "Epoch 19/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1902 - val_loss: -3.1898\n",
      "Epoch 20/100\n",
      "116/116 [==============================] - 1s 5ms/step - loss: -3.1902 - val_loss: -3.1872\n",
      "Epoch 21/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1902 - val_loss: -3.1898\n",
      "Epoch 22/100\n",
      "116/116 [==============================] - 1s 5ms/step - loss: -3.1903 - val_loss: -3.1864\n",
      "Epoch 23/100\n",
      "116/116 [==============================] - 1s 4ms/step - loss: -3.1902 - val_loss: -3.1895\n",
      "Epoch 24/100\n",
      "116/116 [==============================] - 1s 5ms/step - loss: -3.1903 - val_loss: -3.1871\n",
      "Epoch 25/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1903 - val_loss: -3.1885\n",
      "Epoch 26/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1903 - val_loss: -3.1879\n",
      "Epoch 27/100\n",
      "116/116 [==============================] - 1s 4ms/step - loss: -3.1903 - val_loss: -3.1892\n",
      "Epoch 28/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1903 - val_loss: -3.1896\n",
      "Epoch 29/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1903 - val_loss: -3.1892\n",
      "Epoch 30/100\n",
      "116/116 [==============================] - 1s 5ms/step - loss: -3.1903 - val_loss: -3.1882\n",
      "Epoch 31/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1903 - val_loss: -3.1895\n",
      "Epoch 32/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1904 - val_loss: -3.1897\n",
      "Epoch 33/100\n",
      "116/116 [==============================] - 1s 4ms/step - loss: -3.1903 - val_loss: -3.1898\n",
      "Epoch 34/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1903 - val_loss: -3.1885\n",
      "Epoch 35/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1904 - val_loss: -3.1883\n",
      "Epoch 36/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1904 - val_loss: -3.1897\n",
      "Epoch 37/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1905 - val_loss: -3.1897\n",
      "Epoch 38/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1904 - val_loss: -3.1880\n",
      "Epoch 39/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1903 - val_loss: -3.1888\n",
      "Epoch 40/100\n",
      "116/116 [==============================] - 1s 5ms/step - loss: -3.1904 - val_loss: -3.1893\n",
      "Epoch 41/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1904 - val_loss: -3.1900\n",
      "Epoch 42/100\n",
      "116/116 [==============================] - 1s 5ms/step - loss: -3.1904 - val_loss: -3.1896\n",
      "Epoch 43/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1904 - val_loss: -3.1890\n",
      "Epoch 44/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1904 - val_loss: -3.1898\n",
      "Epoch 45/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1904 - val_loss: -3.1896\n",
      "Epoch 46/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1905 - val_loss: -3.1892\n",
      "Epoch 47/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1904 - val_loss: -3.1892\n",
      "Epoch 48/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1904 - val_loss: -3.1900\n",
      "Epoch 49/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1905 - val_loss: -3.1895\n",
      "Epoch 50/100\n",
      "116/116 [==============================] - 1s 4ms/step - loss: -3.1905 - val_loss: -3.1893\n",
      "Epoch 51/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1905 - val_loss: -3.1888\n",
      "Epoch 52/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1904 - val_loss: -3.1898\n",
      "Epoch 53/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1905 - val_loss: -3.1888\n",
      "Epoch 54/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1905 - val_loss: -3.1896\n",
      "Epoch 55/100\n",
      "116/116 [==============================] - 1s 4ms/step - loss: -3.1904 - val_loss: -3.1881\n",
      "Epoch 56/100\n",
      "116/116 [==============================] - 1s 5ms/step - loss: -3.1905 - val_loss: -3.1889\n",
      "Epoch 57/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1905 - val_loss: -3.1891\n",
      "Epoch 58/100\n",
      "116/116 [==============================] - 1s 5ms/step - loss: -3.1905 - val_loss: -3.1893\n",
      "Epoch 59/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1906 - val_loss: -3.1894\n",
      "Epoch 60/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1905 - val_loss: -3.1888\n",
      "Epoch 61/100\n",
      "116/116 [==============================] - 1s 5ms/step - loss: -3.1905 - val_loss: -3.1891\n",
      "Epoch 62/100\n",
      "116/116 [==============================] - 1s 4ms/step - loss: -3.1905 - val_loss: -3.1896\n",
      "Epoch 63/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1906 - val_loss: -3.1899\n",
      "Epoch 64/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1905 - val_loss: -3.1899\n",
      "Epoch 65/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1905 - val_loss: -3.1892\n",
      "Epoch 66/100\n",
      "116/116 [==============================] - 1s 4ms/step - loss: -3.1905 - val_loss: -3.1894\n",
      "Epoch 67/100\n",
      "116/116 [==============================] - 1s 4ms/step - loss: -3.1905 - val_loss: -3.1902\n",
      "Epoch 68/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1905 - val_loss: -3.1894\n",
      "Epoch 69/100\n",
      "116/116 [==============================] - 1s 4ms/step - loss: -3.1905 - val_loss: -3.1898\n",
      "Epoch 70/100\n",
      "116/116 [==============================] - 1s 5ms/step - loss: -3.1905 - val_loss: -3.1893\n",
      "Epoch 71/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1905 - val_loss: -3.1900\n",
      "Epoch 72/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1905 - val_loss: -3.1886\n",
      "Epoch 73/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1906 - val_loss: -3.1895\n",
      "Epoch 74/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1906 - val_loss: -3.1900\n",
      "Epoch 75/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1906 - val_loss: -3.1900\n",
      "Epoch 76/100\n",
      "116/116 [==============================] - 1s 5ms/step - loss: -3.1906 - val_loss: -3.1896\n",
      "Epoch 77/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1905 - val_loss: -3.1894\n",
      "Epoch 78/100\n",
      "116/116 [==============================] - 1s 5ms/step - loss: -3.1905 - val_loss: -3.1898\n",
      "Epoch 79/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1906 - val_loss: -3.1898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1906 - val_loss: -3.1899\n",
      "Epoch 81/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1905 - val_loss: -3.1899\n",
      "Epoch 82/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1906 - val_loss: -3.1897\n",
      "Epoch 83/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1906 - val_loss: -3.1901\n",
      "Epoch 84/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1906 - val_loss: -3.1901\n",
      "Epoch 85/100\n",
      "116/116 [==============================] - 1s 5ms/step - loss: -3.1907 - val_loss: -3.1900\n",
      "Epoch 86/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1906 - val_loss: -3.1893\n",
      "Epoch 87/100\n",
      "116/116 [==============================] - 1s 5ms/step - loss: -3.1906 - val_loss: -3.1900\n",
      "Epoch 88/100\n",
      "116/116 [==============================] - 1s 5ms/step - loss: -3.1906 - val_loss: -3.1896\n",
      "Epoch 89/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1906 - val_loss: -3.1901\n",
      "Epoch 90/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1906 - val_loss: -3.1895\n",
      "Epoch 91/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1906 - val_loss: -3.1893\n",
      "Epoch 92/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1906 - val_loss: -3.1898\n",
      "Epoch 93/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1906 - val_loss: -3.1896\n",
      "Epoch 94/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1907 - val_loss: -3.1891\n",
      "Epoch 95/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1906 - val_loss: -3.1899\n",
      "Epoch 96/100\n",
      "116/116 [==============================] - 1s 4ms/step - loss: -3.1906 - val_loss: -3.1896\n",
      "Epoch 97/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1906 - val_loss: -3.1899\n",
      "Epoch 98/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1906 - val_loss: -3.1897\n",
      "Epoch 99/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1906 - val_loss: -3.1895\n",
      "Epoch 100/100\n",
      "116/116 [==============================] - 0s 4ms/step - loss: -3.1906 - val_loss: -3.1897\n"
     ]
    }
   ],
   "source": [
    "my_train = my_model.fit(x_train, y_train, batch_size=1000,epochs=100,verbose=1,validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "760392eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 102/620 [00:00<00:00, 1011.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done embedding fb-pages-food!\n",
      "Calculating landmarks distance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 620/620 [00:00<00:00, 1008.98it/s]\n",
      "  7%|▋         | 45/620 [00:00<00:01, 441.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done landmarks distance!\n",
      "creating datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 620/620 [00:00<00:00, 688.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 rows removed from the dataset\n",
      "shapes of train: ((115133, 128), (115133,)), valid: ((28784, 128), (28784,)), test: ((47973, 128), (47973,))\n"
     ]
    }
   ],
   "source": [
    "data = create_train_val_test_sets('fb-pages-food',False,False,1,\"top_edges\")\n",
    "x_train, y_train = data[\"x_train\"].astype(np.float32), data[\"y_train\"].astype(np.float32)\n",
    "x_valid, y_valid = data[\"x_val\"].astype(np.float32),data['y_val'].astype(np.float32)\n",
    "x_test, y_test = data[\"x_test\"].astype(np.float32), data[\"y_test\"].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "26c3e258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6769584"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test,(my_model.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a03f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3521d8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.,  3.,  6., ..., 11.,  6.,  6.], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f46666d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 124/124 [00:00<00:00, 1036.62it/s]\n",
      "  0%|          | 0/124 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done embedding fb-pages-food!\n",
      "Calculating landmarks distance...\n",
      "Done landmarks distance!\n",
      "creating datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 124/124 [00:00<00:00, 502.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 rows removed from the dataset\n",
      "shapes of train: ((41476, 128), (41476,)), valid: ((10369, 128), (10369,)), test: ((17282, 128), (17282,))\n",
      "model loaded into device= cuda:0\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 200]          25,800\n",
      "       BatchNorm1d-2                  [-1, 200]             400\n",
      "          Softplus-3                  [-1, 200]               0\n",
      "            Linear-4                  [-1, 100]          20,100\n",
      "       BatchNorm1d-5                  [-1, 100]             200\n",
      "          Softplus-6                  [-1, 100]               0\n",
      "            Linear-7                   [-1, 50]           5,050\n",
      "       BatchNorm1d-8                   [-1, 50]             100\n",
      "          Softplus-9                   [-1, 50]               0\n",
      "           Linear-10                    [-1, 1]              51\n",
      "         Softplus-11                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 51,701\n",
      "Trainable params: 51,701\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.20\n",
      "Estimated Total Size (MB): 0.21\n",
      "----------------------------------------------------------------\n",
      "lr scheduler type: <torch.optim.lr_scheduler.CyclicLR object at 0x2b4ee6fb6d00>\n",
      "1e-05\n",
      "returned 66\n",
      "total epochs= 150\n",
      "lr-check 0.2154434690031885\n",
      "0 > Best val_loss model saved: -1.9375\n",
      "Saving test scores\n",
      "-1.9263884109609268\n",
      "0.18194117647058825\n",
      "17282 17000\n",
      "epoch:0 -> train_loss=-1.76452,val_loss=-1.93747 - 0.6413240432739258\n",
      "1 > Best val_loss model saved: -2.1499\n",
      "Saving test scores\n",
      "-2.1378705186002396\n",
      "0.22\n",
      "17282 17000\n",
      "epoch:1 -> train_loss=-2.02291,val_loss=-2.14992 - 0.8223488330841064\n",
      "2 > Best val_loss model saved: -2.2792\n",
      "Saving test scores\n",
      "-2.2678108110147366\n",
      "0.2475294117647059\n",
      "17282 17000\n",
      "epoch:2 -> train_loss=-2.18396,val_loss=-2.27922 - 0.6545886993408203\n",
      "3 > Best val_loss model saved: -2.3861\n",
      "Saving test scores\n",
      "-2.3762356954462387\n",
      "0.27894117647058825\n",
      "17282 17000\n",
      "epoch:3 -> train_loss=-2.3041,val_loss=-2.3861 - 0.6387336254119873\n",
      "4 > Best val_loss model saved: -2.4727\n",
      "Saving test scores\n",
      "-2.4640564778271843\n",
      "0.34241176470588236\n",
      "17282 17000\n",
      "epoch:4 -> train_loss=-2.4028,val_loss=-2.47271 - 0.836418628692627\n",
      "5 > Best val_loss model saved: -2.5417\n",
      "Saving test scores\n",
      "-2.534125741790323\n",
      "0.45658823529411763\n",
      "17282 17000\n",
      "epoch:5 -> train_loss=-2.48911,val_loss=-2.54175 - 0.6427757740020752\n",
      "6 > Best val_loss model saved: -2.561\n",
      "Saving test scores\n",
      "-2.5576582936679615\n",
      "0.49623529411764705\n",
      "17282 17000\n",
      "epoch:6 -> train_loss=-2.53322,val_loss=-2.56099 - 0.8276610374450684\n",
      "7 > Best val_loss model saved: -2.577\n",
      "Saving test scores\n",
      "-2.5752852453904995\n",
      "0.5260588235294118\n",
      "17282 17000\n",
      "epoch:7 -> train_loss=-2.55475,val_loss=-2.577 - 0.6503674983978271\n",
      "8 > Best val_loss model saved: -2.5881\n",
      "Saving test scores\n",
      "-2.5860834191827213\n",
      "0.546235294117647\n",
      "17282 17000\n",
      "epoch:8 -> train_loss=-2.5702,val_loss=-2.5881 - 0.8260905742645264\n",
      "9 > Best val_loss model saved: -2.5941\n",
      "Saving test scores\n",
      "-2.592293290530934\n",
      "0.5625294117647058\n",
      "17282 17000\n",
      "epoch:9 -> train_loss=-2.57921,val_loss=-2.59414 - 0.6565396785736084\n",
      "10 > Best val_loss model saved: -2.5992\n",
      "Saving test scores\n",
      "-2.5978930767844703\n",
      "0.572764705882353\n",
      "17282 17000\n",
      "epoch:10 -> train_loss=-2.5857,val_loss=-2.59919 - 0.6524462699890137\n",
      "11 > Best val_loss model saved: -2.6054\n",
      "Saving test scores\n",
      "-2.6049302956637215\n",
      "0.5875882352941176\n",
      "17282 17000\n",
      "epoch:11 -> train_loss=-2.59093,val_loss=-2.60537 - 0.8345906734466553\n",
      "12 > Best val_loss model saved: -2.6084\n",
      "Saving test scores\n",
      "-2.608002641621758\n",
      "0.5947058823529412\n",
      "17282 17000\n",
      "epoch:12 -> train_loss=-2.59528,val_loss=-2.60839 - 0.6499497890472412\n",
      "13 > Best val_loss model saved: -2.6106\n",
      "Saving test scores\n",
      "-2.610238895696752\n",
      "0.6001176470588235\n",
      "17282 17000\n",
      "epoch:13 -> train_loss=-2.59864,val_loss=-2.61056 - 0.8169052600860596\n",
      "14 > Best val_loss model saved: -2.6121\n",
      "Saving test scores\n",
      "-2.612075553220861\n",
      "0.6032352941176471\n",
      "17282 17000\n",
      "epoch:14 -> train_loss=-2.60205,val_loss=-2.61213 - 0.6456127166748047\n",
      "15 > Best val_loss model saved: -2.6124\n",
      "Saving test scores\n",
      "-2.61269833761103\n",
      "0.6068823529411764\n",
      "17282 17000\n",
      "epoch:15 -> train_loss=-2.60396,val_loss=-2.61243 - 0.8184397220611572\n",
      "16 > Best val_loss model saved: -2.6133\n",
      "Saving test scores\n",
      "-2.6133426077225628\n",
      "0.6081764705882353\n",
      "17282 17000\n",
      "epoch:16 -> train_loss=-2.60492,val_loss=-2.6133 - 0.6566781997680664\n",
      "17 > Best val_loss model saved: -2.6147\n",
      "Saving test scores\n",
      "-2.6145361100926117\n",
      "0.612\n",
      "17282 17000\n",
      "epoch:17 -> train_loss=-2.60528,val_loss=-2.61473 - 0.6538834571838379\n",
      "18 > Best val_loss model saved: -2.6162\n",
      "Saving test scores\n",
      "-2.615941952256595\n",
      "0.6191176470588236\n",
      "17282 17000\n",
      "epoch:18 -> train_loss=-2.60678,val_loss=-2.61622 - 0.8206777572631836\n",
      "19 > Best val_loss model saved: -2.6182\n",
      "Saving test scores\n",
      "-2.618035176221062\n",
      "0.6234117647058823\n",
      "17282 17000\n",
      "epoch:19 -> train_loss=-2.60878,val_loss=-2.61824 - 0.6561496257781982\n",
      "20 > Best val_loss model saved: -2.6194\n",
      "Saving test scores\n",
      "-2.619131747414084\n",
      "0.6195882352941177\n",
      "17282 17000\n",
      "epoch:20 -> train_loss=-2.6111,val_loss=-2.61943 - 0.8221056461334229\n",
      "21 > Best val_loss model saved: -2.6215\n",
      "Saving test scores\n",
      "-2.6211600724388573\n",
      "0.6269411764705882\n",
      "17282 17000\n",
      "epoch:21 -> train_loss=-2.61358,val_loss=-2.62148 - 0.6497149467468262\n",
      "22 > Best val_loss model saved: -2.6243\n",
      "Saving test scores\n",
      "-2.6241664676105274\n",
      "0.6419411764705882\n",
      "17282 17000\n",
      "epoch:22 -> train_loss=-2.61613,val_loss=-2.62433 - 0.642888069152832\n",
      "23 > Best val_loss model saved: -2.6266\n",
      "Saving test scores\n",
      "-2.6266772817162907\n",
      "0.6518235294117647\n",
      "17282 17000\n",
      "epoch:23 -> train_loss=-2.6186,val_loss=-2.62656 - 0.8246326446533203\n",
      "24 > Best val_loss model saved: -2.6283\n",
      "Saving test scores\n",
      "-2.628515362739563\n",
      "0.6606470588235294\n",
      "17282 17000\n",
      "epoch:24 -> train_loss=-2.62103,val_loss=-2.62831 - 0.6536767482757568\n",
      "25 > Best val_loss model saved: -2.6293\n",
      "Saving test scores\n",
      "-2.6295832676046036\n",
      "0.6641176470588235\n",
      "17282 17000\n",
      "epoch:25 -> train_loss=-2.62329,val_loss=-2.62932 - 0.8299229145050049\n",
      "26 > Best val_loss model saved: -2.6295\n",
      "Saving test scores\n",
      "-2.6297727122026333\n",
      "0.6634705882352941\n",
      "17282 17000\n",
      "epoch:26 -> train_loss=-2.62511,val_loss=-2.62952 - 0.6566412448883057\n",
      "27 > Best val_loss model saved: -2.6312\n",
      "Saving test scores\n",
      "-2.6315968527513394\n",
      "0.6761176470588235\n",
      "17282 17000\n",
      "epoch:27 -> train_loss=-2.62665,val_loss=-2.63123 - 0.8282639980316162\n",
      "28 > Best val_loss model saved: -2.6335\n",
      "Saving test scores\n",
      "-2.633836690117331\n",
      "0.688\n",
      "17282 17000\n",
      "epoch:28 -> train_loss=-2.6284,val_loss=-2.6335 - 0.6479711532592773\n",
      "29 > Best val_loss model saved: -2.6349\n",
      "Saving test scores\n",
      "-2.635161617222954\n",
      "0.6979411764705883\n",
      "17282 17000\n",
      "epoch:29 -> train_loss=-2.63008,val_loss=-2.63493 - 0.654332160949707\n",
      "30 > Best val_loss model saved: -2.6357\n",
      "Saving test scores\n",
      "-2.635919816353742\n",
      "0.7031764705882353\n",
      "17282 17000\n",
      "epoch:30 -> train_loss=-2.63131,val_loss=-2.63571 - 0.8261785507202148\n",
      "31 > Best val_loss model saved: -2.6358\n",
      "Saving test scores\n",
      "-2.636333809179418\n",
      "0.706764705882353\n",
      "17282 17000\n",
      "epoch:31 -> train_loss=-2.6323,val_loss=-2.63583 - 0.6551659107208252\n",
      "32 > Best val_loss model saved: -2.6361\n",
      "Saving test scores\n",
      "-2.636342504445244\n",
      "0.7071176470588235\n",
      "17282 17000\n",
      "epoch:32 -> train_loss=-2.63272,val_loss=-2.63607 - 0.8281440734863281\n",
      "Saving test scores\n",
      "-2.6361641673480762\n",
      "0.7012941176470588\n",
      "17282 17000\n",
      "epoch:33 -> train_loss=-2.63225,val_loss=-2.63598 - 0.6534700393676758\n",
      "Saving test scores\n",
      "-2.6354443676331463\n",
      "0.6965294117647058\n",
      "17282 17000\n",
      "epoch:34 -> train_loss=-2.63229,val_loss=-2.63515 - 0.6521999835968018\n",
      "Saving test scores\n",
      "-2.6351214857662426\n",
      "0.6921764705882353\n",
      "17282 17000\n",
      "epoch:35 -> train_loss=-2.63266,val_loss=-2.6348 - 0.8282232284545898\n",
      "36 > Best val_loss model saved: -2.6364\n",
      "Saving test scores\n",
      "-2.6368996886646046\n",
      "0.7041764705882353\n",
      "17282 17000\n",
      "epoch:36 -> train_loss=-2.63339,val_loss=-2.63645 - 0.6454324722290039\n",
      "37 > Best val_loss model saved: -2.6375\n",
      "Saving test scores\n",
      "-2.638128462959738\n",
      "0.7132941176470589\n",
      "17282 17000\n",
      "epoch:37 -> train_loss=-2.63422,val_loss=-2.63748 - 0.8281714916229248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving test scores\n",
      "-2.637965069097631\n",
      "0.7108235294117647\n",
      "17282 17000\n",
      "epoch:38 -> train_loss=-2.63504,val_loss=-2.63711 - 0.655287504196167\n",
      "Saving test scores\n",
      "-2.637586032643038\n",
      "0.7034705882352941\n",
      "17282 17000\n",
      "epoch:39 -> train_loss=-2.63592,val_loss=-2.63665 - 0.8234348297119141\n",
      "40 > Best val_loss model saved: -2.6383\n",
      "Saving test scores\n",
      "-2.6391837456647087\n",
      "0.7139411764705882\n",
      "17282 17000\n",
      "epoch:40 -> train_loss=-2.63691,val_loss=-2.63831 - 0.6556172370910645\n",
      "41 > Best val_loss model saved: -2.6403\n",
      "Saving test scores\n",
      "-2.6410795941072354\n",
      "0.7311176470588235\n",
      "17282 17000\n",
      "epoch:41 -> train_loss=-2.63792,val_loss=-2.64033 - 0.657451868057251\n",
      "42 > Best val_loss model saved: -2.6415\n",
      "Saving test scores\n",
      "-2.6420578605988445\n",
      "0.7364705882352941\n",
      "17282 17000\n",
      "epoch:42 -> train_loss=-2.6388,val_loss=-2.64149 - 0.8152415752410889\n",
      "43 > Best val_loss model saved: -2.6417\n",
      "Saving test scores\n",
      "-2.6421549600713394\n",
      "0.7359411764705882\n",
      "17282 17000\n",
      "epoch:43 -> train_loss=-2.63953,val_loss=-2.64165 - 0.6414012908935547\n",
      "44 > Best val_loss model saved: -2.6423\n",
      "Saving test scores\n",
      "-2.6428833498674282\n",
      "0.7418823529411764\n",
      "17282 17000\n",
      "epoch:44 -> train_loss=-2.64025,val_loss=-2.64229 - 0.8191165924072266\n",
      "45 > Best val_loss model saved: -2.6436\n",
      "Saving test scores\n",
      "-2.6441791548448452\n",
      "0.7547058823529412\n",
      "17282 17000\n",
      "epoch:45 -> train_loss=-2.64124,val_loss=-2.64356 - 0.654364824295044\n",
      "46 > Best val_loss model saved: -2.6445\n",
      "Saving test scores\n",
      "-2.6449882002437817\n",
      "0.7586470588235295\n",
      "17282 17000\n",
      "epoch:46 -> train_loss=-2.64201,val_loss=-2.64445 - 0.6563730239868164\n",
      "47 > Best val_loss model saved: -2.6448\n",
      "Saving test scores\n",
      "-2.645332567832049\n",
      "0.7634705882352941\n",
      "17282 17000\n",
      "epoch:47 -> train_loss=-2.6427,val_loss=-2.64476 - 0.8223795890808105\n",
      "Saving test scores\n",
      "-2.64520903895883\n",
      "0.7600588235294118\n",
      "17282 17000\n",
      "epoch:48 -> train_loss=-2.64304,val_loss=-2.64467 - 0.6453139781951904\n",
      "Saving test scores\n",
      "-2.6447011442745434\n",
      "0.7552941176470588\n",
      "17282 17000\n",
      "epoch:49 -> train_loss=-2.64251,val_loss=-2.64409 - 0.8168132305145264\n",
      "Saving test scores\n",
      "-2.6444191652185776\n",
      "0.7521176470588236\n",
      "17282 17000\n",
      "epoch:50 -> train_loss=-2.64241,val_loss=-2.64384 - 0.6522395610809326\n",
      "Saving test scores\n",
      "-2.644682484514573\n",
      "0.7503529411764706\n",
      "17282 17000\n",
      "epoch:51 -> train_loss=-2.64261,val_loss=-2.64409 - 0.8137905597686768\n",
      "Saving test scores\n",
      "-2.645145093693453\n",
      "0.7521176470588236\n",
      "17282 17000\n",
      "epoch:52 -> train_loss=-2.64292,val_loss=-2.64446 - 0.6523559093475342\n",
      "Saving test scores\n",
      "-2.644485494669746\n",
      "0.7480588235294118\n",
      "17282 17000\n",
      "epoch:53 -> train_loss=-2.6432,val_loss=-2.64369 - 0.8144357204437256\n",
      "Saving test scores\n",
      "-2.644487563301535\n",
      "0.7458823529411764\n",
      "17282 17000\n",
      "epoch:54 -> train_loss=-2.64358,val_loss=-2.64372 - 0.651221513748169\n",
      "Saving test scores\n",
      "-2.6450359330457798\n",
      "0.7513529411764706\n",
      "17282 17000\n",
      "epoch:55 -> train_loss=-2.64398,val_loss=-2.64435 - 0.6530592441558838\n",
      "Saving test scores\n",
      "-2.6454398702172672\n",
      "0.7541764705882353\n",
      "17282 17000\n",
      "epoch:56 -> train_loss=-2.64446,val_loss=-2.6447 - 0.8202724456787109\n",
      "57 > Best val_loss model saved: -2.6453\n",
      "Saving test scores\n",
      "-2.6461394043529736\n",
      "0.7614705882352941\n",
      "17282 17000\n",
      "epoch:57 -> train_loss=-2.64502,val_loss=-2.64534 - 0.6522879600524902\n",
      "58 > Best val_loss model saved: -2.6467\n",
      "Saving test scores\n",
      "-2.6474650256774006\n",
      "0.7737058823529411\n",
      "17282 17000\n",
      "epoch:58 -> train_loss=-2.6455,val_loss=-2.64672 - 0.830590009689331\n",
      "59 > Best val_loss model saved: -2.6471\n",
      "Saving test scores\n",
      "-2.6477401817546173\n",
      "0.7753529411764706\n",
      "17282 17000\n",
      "epoch:59 -> train_loss=-2.64592,val_loss=-2.64708 - 0.6561679840087891\n",
      "60 > Best val_loss model saved: -2.6474\n",
      "Saving test scores\n",
      "-2.6481181032517376\n",
      "0.7797647058823529\n",
      "17282 17000\n",
      "epoch:60 -> train_loss=-2.6463,val_loss=-2.64744 - 0.6420066356658936\n",
      "61 > Best val_loss model saved: -2.6483\n",
      "Saving test scores\n",
      "-2.6490712095709408\n",
      "0.7866470588235294\n",
      "17282 17000\n",
      "epoch:61 -> train_loss=-2.64686,val_loss=-2.64834 - 0.8198118209838867\n",
      "62 > Best val_loss model saved: -2.649\n",
      "Saving test scores\n",
      "-2.649725451188929\n",
      "0.7917647058823529\n",
      "17282 17000\n",
      "epoch:62 -> train_loss=-2.64744,val_loss=-2.64901 - 0.6532118320465088\n",
      "63 > Best val_loss model saved: -2.6493\n",
      "Saving test scores\n",
      "-2.6500014978296615\n",
      "0.7971176470588235\n",
      "17282 17000\n",
      "epoch:63 -> train_loss=-2.648,val_loss=-2.64926 - 0.8286659717559814\n",
      "Saving test scores\n",
      "-2.6497983932495117\n",
      "0.7909411764705883\n",
      "17282 17000\n",
      "epoch:64 -> train_loss=-2.64829,val_loss=-2.64911 - 0.6493918895721436\n",
      "Saving test scores\n",
      "-2.6490236310397877\n",
      "0.7874117647058824\n",
      "17282 17000\n",
      "epoch:65 -> train_loss=-2.64776,val_loss=-2.64828 - 0.6527247428894043\n",
      "Saving test scores\n",
      "-2.6488242009106804\n",
      "0.7869411764705883\n",
      "17282 17000\n",
      "epoch:66 -> train_loss=-2.64756,val_loss=-2.64812 - 0.8257324695587158\n",
      "Saving test scores\n",
      "-2.6490700946134678\n",
      "0.7885294117647059\n",
      "17282 17000\n",
      "epoch:67 -> train_loss=-2.64768,val_loss=-2.64835 - 0.6413445472717285\n",
      "Saving test scores\n",
      "-2.648702214745914\n",
      "0.785\n",
      "17282 17000\n",
      "epoch:68 -> train_loss=-2.64776,val_loss=-2.64792 - 0.8181688785552979\n",
      "Saving test scores\n",
      "-2.6480955095852123\n",
      "0.778235294117647\n",
      "17282 17000\n",
      "epoch:69 -> train_loss=-2.6479,val_loss=-2.6473 - 0.649648904800415\n",
      "Saving test scores\n",
      "-2.648359039250542\n",
      "0.7812941176470588\n",
      "17282 17000\n",
      "epoch:70 -> train_loss=-2.64807,val_loss=-2.6476 - 0.659243106842041\n",
      "Saving test scores\n",
      "-2.6489591247895183\n",
      "0.7874117647058824\n",
      "17282 17000\n",
      "epoch:71 -> train_loss=-2.64825,val_loss=-2.64827 - 0.8427724838256836\n",
      "Saving test scores\n",
      "-2.649024283184725\n",
      "0.788235294117647\n",
      "17282 17000\n",
      "epoch:72 -> train_loss=-2.64854,val_loss=-2.64828 - 0.6517400741577148\n",
      "Saving test scores\n",
      "-2.6488662116667805\n",
      "0.7861176470588235\n",
      "17282 17000\n",
      "epoch:73 -> train_loss=-2.64898,val_loss=-2.64808 - 0.816645622253418\n",
      "Saving test scores\n",
      "-2.6499530217226814\n",
      "0.7976470588235294\n",
      "17282 17000\n",
      "epoch:74 -> train_loss=-2.6493,val_loss=-2.64917 - 0.6514172554016113\n",
      "75 > Best val_loss model saved: -2.65\n",
      "Saving test scores\n",
      "-2.650697855388417\n",
      "0.805235294117647\n",
      "17282 17000\n",
      "epoch:75 -> train_loss=-2.64959,val_loss=-2.64997 - 0.856989860534668\n",
      "Saving test scores\n",
      "-2.6503390003653133\n",
      "0.8005294117647059\n",
      "17282 17000\n",
      "epoch:76 -> train_loss=-2.64982,val_loss=-2.6496 - 0.6518650054931641\n",
      "77 > Best val_loss model saved: -2.6507\n",
      "Saving test scores\n",
      "-2.6514551218818214\n",
      "0.8068823529411765\n",
      "17282 17000\n",
      "epoch:77 -> train_loss=-2.65024,val_loss=-2.6507 - 0.7849218845367432\n",
      "78 > Best val_loss model saved: -2.6514\n",
      "Saving test scores\n",
      "-2.6521352529525757\n",
      "0.8139411764705883\n",
      "17282 17000\n",
      "epoch:78 -> train_loss=-2.65069,val_loss=-2.65138 - 0.6534900665283203\n",
      "79 > Best val_loss model saved: -2.6516\n",
      "Saving test scores\n",
      "-2.6522902460659252\n",
      "0.8154117647058824\n",
      "17282 17000\n",
      "epoch:79 -> train_loss=-2.65114,val_loss=-2.65157 - 0.6480684280395508\n",
      "Saving test scores\n",
      "-2.6521302742116593\n",
      "0.8127058823529412\n",
      "17282 17000\n",
      "epoch:80 -> train_loss=-2.65145,val_loss=-2.65143 - 0.8021218776702881\n",
      "Saving test scores\n",
      "-2.651321102591122\n",
      "0.8065882352941176\n",
      "17282 17000\n",
      "epoch:81 -> train_loss=-2.65091,val_loss=-2.65059 - 0.6484236717224121\n",
      "Saving test scores\n",
      "-2.6509845046436085\n",
      "0.8058823529411765\n",
      "17282 17000\n",
      "epoch:82 -> train_loss=-2.65078,val_loss=-2.65026 - 0.7751033306121826\n",
      "Saving test scores\n",
      "-2.6512662803425506\n",
      "0.8089411764705883\n",
      "17282 17000\n",
      "epoch:83 -> train_loss=-2.65083,val_loss=-2.65055 - 0.6477804183959961\n",
      "Saving test scores\n",
      "-2.6510078065535603\n",
      "0.8077647058823529\n",
      "17282 17000\n",
      "epoch:84 -> train_loss=-2.65087,val_loss=-2.65029 - 0.6365160942077637\n",
      "Saving test scores\n",
      "-2.6509703327627743\n",
      "0.8064117647058824\n",
      "17282 17000\n",
      "epoch:85 -> train_loss=-2.65096,val_loss=-2.65027 - 0.7719008922576904\n",
      "Saving test scores\n",
      "-2.651398139841416\n",
      "0.8079411764705883\n",
      "17282 17000\n",
      "epoch:86 -> train_loss=-2.65106,val_loss=-2.65074 - 0.6490161418914795\n",
      "Saving test scores\n",
      "-2.6516402749454273\n",
      "0.8094705882352942\n",
      "17282 17000\n",
      "epoch:87 -> train_loss=-2.65115,val_loss=-2.65104 - 0.7754473686218262\n",
      "Saving test scores\n",
      "-2.6517482925863827\n",
      "0.8119411764705883\n",
      "17282 17000\n",
      "epoch:88 -> train_loss=-2.65138,val_loss=-2.65112 - 0.649585485458374\n",
      "Saving test scores\n",
      "-2.6514190505532658\n",
      "0.8108823529411765\n",
      "17282 17000\n",
      "epoch:89 -> train_loss=-2.65177,val_loss=-2.65079 - 0.648390531539917\n",
      "Saving test scores\n",
      "-2.6512565542669857\n",
      "0.8099411764705883\n",
      "17282 17000\n",
      "epoch:90 -> train_loss=-2.65205,val_loss=-2.65059 - 0.7719268798828125\n",
      "Saving test scores\n",
      "-2.651530805756064\n",
      "0.8168235294117647\n",
      "17282 17000\n",
      "epoch:91 -> train_loss=-2.65225,val_loss=-2.65086 - 0.6413123607635498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving test scores\n",
      "-2.6519421829896817\n",
      "0.816764705882353\n",
      "17282 17000\n",
      "epoch:92 -> train_loss=-2.65241,val_loss=-2.65124 - 0.765221357345581\n",
      "93 > Best val_loss model saved: -2.6525\n",
      "Saving test scores\n",
      "-2.6532313543207504\n",
      "0.8275294117647058\n",
      "17282 17000\n",
      "epoch:93 -> train_loss=-2.65274,val_loss=-2.65246 - 0.6528186798095703\n",
      "94 > Best val_loss model saved: -2.6531\n",
      "Saving test scores\n",
      "-2.6537935242933384\n",
      "0.8311764705882353\n",
      "17282 17000\n",
      "epoch:94 -> train_loss=-2.65316,val_loss=-2.65307 - 0.7973959445953369\n",
      "95 > Best val_loss model saved: -2.6532\n",
      "Saving test scores\n",
      "-2.6538493002162262\n",
      "0.8313529411764706\n",
      "17282 17000\n",
      "epoch:95 -> train_loss=-2.65356,val_loss=-2.65322 - 0.6408627033233643\n",
      "Saving test scores\n",
      "-2.6537101409014534\n",
      "0.8322352941176471\n",
      "17282 17000\n",
      "epoch:96 -> train_loss=-2.65388,val_loss=-2.65303 - 0.7686872482299805\n",
      "Saving test scores\n",
      "-2.653258204460144\n",
      "0.8261764705882353\n",
      "17282 17000\n",
      "epoch:97 -> train_loss=-2.65331,val_loss=-2.65257 - 0.6500697135925293\n",
      "Saving test scores\n",
      "-2.652528601534226\n",
      "0.8219411764705883\n",
      "17282 17000\n",
      "epoch:98 -> train_loss=-2.6532,val_loss=-2.65182 - 0.6414825916290283\n",
      "Saving test scores\n",
      "-2.6523339117274567\n",
      "0.8219411764705883\n",
      "17282 17000\n",
      "epoch:99 -> train_loss=-2.65325,val_loss=-2.65166 - 0.7653038501739502\n",
      "Saving test scores\n",
      "-2.651905613787034\n",
      "0.8183529411764706\n",
      "17282 17000\n",
      "epoch:100 -> train_loss=-2.65328,val_loss=-2.65123 - 0.6463718414306641\n",
      "Saving test scores\n",
      "-2.652092793408562\n",
      "0.8178823529411765\n",
      "17282 17000\n",
      "epoch:101 -> train_loss=-2.65335,val_loss=-2.65146 - 0.7739946842193604\n",
      "Saving test scores\n",
      "-2.652354401700637\n",
      "0.8180588235294117\n",
      "17282 17000\n",
      "epoch:102 -> train_loss=-2.65338,val_loss=-2.65179 - 0.6499226093292236\n",
      "Saving test scores\n",
      "-2.6524980979807236\n",
      "0.8188823529411765\n",
      "17282 17000\n",
      "epoch:103 -> train_loss=-2.65339,val_loss=-2.65199 - 0.6476430892944336\n",
      "Saving test scores\n",
      "-2.6527461023891674\n",
      "0.8208823529411765\n",
      "17282 17000\n",
      "epoch:104 -> train_loss=-2.65356,val_loss=-2.65224 - 0.7707309722900391\n",
      "Saving test scores\n",
      "-2.652410801719217\n",
      "0.8204117647058824\n",
      "17282 17000\n",
      "epoch:105 -> train_loss=-2.65387,val_loss=-2.6519 - 0.6464743614196777\n",
      "Saving test scores\n",
      "-2.6517582921420826\n",
      "0.8218823529411765\n",
      "17282 17000\n",
      "epoch:106 -> train_loss=-2.6541,val_loss=-2.65121 - 0.763695240020752\n",
      "Saving test scores\n",
      "-2.6526864346335914\n",
      "0.8284705882352941\n",
      "17282 17000\n",
      "epoch:107 -> train_loss=-2.6542,val_loss=-2.65211 - 0.6429522037506104\n",
      "Saving test scores\n",
      "-2.653101752786075\n",
      "0.8262352941176471\n",
      "17282 17000\n",
      "epoch:108 -> train_loss=-2.65427,val_loss=-2.65252 - 0.6526639461517334\n",
      "109 > Best val_loss model saved: -2.6535\n",
      "Saving test scores\n",
      "-2.6541215952704933\n",
      "0.8374705882352941\n",
      "17282 17000\n",
      "epoch:109 -> train_loss=-2.65457,val_loss=-2.6535 - 0.7729074954986572\n",
      "110 > Best val_loss model saved: -2.654\n",
      "Saving test scores\n",
      "-2.6546114683151245\n",
      "0.8431764705882353\n",
      "17282 17000\n",
      "epoch:110 -> train_loss=-2.65494,val_loss=-2.65403 - 0.6510119438171387\n",
      "111 > Best val_loss model saved: -2.6543\n",
      "Saving test scores\n",
      "-2.654745747061337\n",
      "0.8427058823529412\n",
      "17282 17000\n",
      "epoch:111 -> train_loss=-2.65531,val_loss=-2.6543 - 0.7726123332977295\n",
      "Saving test scores\n",
      "-2.654490323627696\n",
      "0.8421176470588235\n",
      "17282 17000\n",
      "epoch:112 -> train_loss=-2.65562,val_loss=-2.65395 - 0.6463449001312256\n",
      "Saving test scores\n",
      "-2.654114646070144\n",
      "0.8374705882352941\n",
      "17282 17000\n",
      "epoch:113 -> train_loss=-2.65504,val_loss=-2.65359 - 0.6394157409667969\n",
      "Saving test scores\n",
      "-2.6534973312826717\n",
      "0.831\n",
      "17282 17000\n",
      "epoch:114 -> train_loss=-2.65493,val_loss=-2.65296 - 0.7713344097137451\n",
      "Saving test scores\n",
      "-2.6532747605267692\n",
      "0.8325882352941176\n",
      "17282 17000\n",
      "epoch:115 -> train_loss=-2.65498,val_loss=-2.65281 - 0.6496002674102783\n",
      "Saving test scores\n",
      "-2.6526985238580143\n",
      "0.8289411764705882\n",
      "17282 17000\n",
      "epoch:116 -> train_loss=-2.65502,val_loss=-2.65226 - 0.7680683135986328\n",
      "Saving test scores\n",
      "-2.652488280745114\n",
      "0.8268235294117647\n",
      "17282 17000\n",
      "epoch:117 -> train_loss=-2.65508,val_loss=-2.65213 - 0.633122444152832\n",
      "Saving test scores\n",
      "-2.652481387643253\n",
      "0.8275882352941176\n",
      "17282 17000\n",
      "epoch:118 -> train_loss=-2.65508,val_loss=-2.65221 - 0.755159854888916\n",
      "Saving test scores\n",
      "-2.652473106103785\n",
      "0.8263529411764706\n",
      "17282 17000\n",
      "epoch:119 -> train_loss=-2.65508,val_loss=-2.65228 - 0.6399338245391846\n",
      "Saving test scores\n",
      "-2.6530878052991977\n",
      "0.8312352941176471\n",
      "17282 17000\n",
      "epoch:120 -> train_loss=-2.6552,val_loss=-2.65288 - 0.7794413566589355\n",
      "Saving test scores\n",
      "-2.653055233113906\n",
      "0.8315294117647059\n",
      "17282 17000\n",
      "epoch:121 -> train_loss=-2.65545,val_loss=-2.65284 - 0.6416058540344238\n",
      "Saving test scores\n",
      "-2.6519972366445206\n",
      "0.8292352941176471\n",
      "17282 17000\n",
      "epoch:122 -> train_loss=-2.65567,val_loss=-2.65176 - 0.6474478244781494\n",
      "Saving test scores\n",
      "-2.653819946681752\n",
      "0.8381176470588235\n",
      "17282 17000\n",
      "epoch:123 -> train_loss=-2.65573,val_loss=-2.65348 - 0.7851831912994385\n",
      "Saving test scores\n",
      "-2.653708710389979\n",
      "0.8338235294117647\n",
      "17282 17000\n",
      "epoch:124 -> train_loss=-2.65577,val_loss=-2.65331 - 0.6450130939483643\n",
      "Saving test scores\n",
      "-2.6546867665122535\n",
      "0.8424117647058823\n",
      "17282 17000\n",
      "epoch:125 -> train_loss=-2.65604,val_loss=-2.65427 - 0.7680513858795166\n",
      "126 > Best val_loss model saved: -2.6547\n",
      "Saving test scores\n",
      "-2.655130190007827\n",
      "0.8472941176470589\n",
      "17282 17000\n",
      "epoch:126 -> train_loss=-2.65637,val_loss=-2.65471 - 0.6538829803466797\n",
      "127 > Best val_loss model saved: -2.6551\n",
      "Saving test scores\n",
      "-2.6553444862365723\n",
      "0.8480588235294118\n",
      "17282 17000\n",
      "epoch:127 -> train_loss=-2.65675,val_loss=-2.65511 - 0.6501929759979248\n",
      "Saving test scores\n",
      "-2.6550070327870987\n",
      "0.8473529411764706\n",
      "17282 17000\n",
      "epoch:128 -> train_loss=-2.65705,val_loss=-2.65463 - 0.7745528221130371\n",
      "Saving test scores\n",
      "-2.654599252869101\n",
      "0.8422941176470589\n",
      "17282 17000\n",
      "epoch:129 -> train_loss=-2.65646,val_loss=-2.65427 - 0.6452114582061768\n",
      "Saving test scores\n",
      "-2.6541633255341472\n",
      "0.8372941176470589\n",
      "17282 17000\n",
      "epoch:130 -> train_loss=-2.65636,val_loss=-2.65381 - 0.7582249641418457\n",
      "Saving test scores\n",
      "-2.6541768382577335\n",
      "0.8404117647058823\n",
      "17282 17000\n",
      "epoch:131 -> train_loss=-2.65641,val_loss=-2.65392 - 0.6522951126098633\n",
      "Saving test scores\n",
      "-2.6535803079605103\n",
      "0.8358235294117647\n",
      "17282 17000\n",
      "epoch:132 -> train_loss=-2.65644,val_loss=-2.65339 - 0.6505794525146484\n",
      "Saving test scores\n",
      "-2.6528105174793915\n",
      "0.8335882352941176\n",
      "17282 17000\n",
      "epoch:133 -> train_loss=-2.65649,val_loss=-2.65274 - 0.7713994979858398\n",
      "Saving test scores\n",
      "-2.6528748273849487\n",
      "0.8321176470588235\n",
      "17282 17000\n",
      "epoch:134 -> train_loss=-2.65648,val_loss=-2.65285 - 0.6487205028533936\n",
      "Saving test scores\n",
      "-2.6528461259954117\n",
      "0.8328235294117647\n",
      "17282 17000\n",
      "epoch:135 -> train_loss=-2.65647,val_loss=-2.6529 - 0.7742002010345459\n",
      "Saving test scores\n",
      "-2.65311159807093\n",
      "0.8334705882352941\n",
      "17282 17000\n",
      "epoch:136 -> train_loss=-2.65657,val_loss=-2.65314 - 0.6405396461486816\n",
      "Saving test scores\n",
      "-2.6529371598187614\n",
      "0.8354117647058823\n",
      "17282 17000\n",
      "epoch:137 -> train_loss=-2.65682,val_loss=-2.65296 - 0.7677502632141113\n",
      "Saving test scores\n",
      "-2.6522703100653255\n",
      "0.8353529411764706\n",
      "17282 17000\n",
      "epoch:138 -> train_loss=-2.657,val_loss=-2.65227 - 0.6386778354644775\n",
      "Saving test scores\n",
      "-2.6547228027792538\n",
      "0.843\n",
      "17282 17000\n",
      "epoch:139 -> train_loss=-2.65703,val_loss=-2.65449 - 0.649315595626831\n",
      "Saving test scores\n",
      "-2.6536346603842342\n",
      "0.8359411764705882\n",
      "17282 17000\n",
      "epoch:140 -> train_loss=-2.65705,val_loss=-2.6533 - 0.7698867321014404\n",
      "Saving test scores\n",
      "-2.654946635751163\n",
      "0.8452352941176471\n",
      "17282 17000\n",
      "epoch:141 -> train_loss=-2.65728,val_loss=-2.65461 - 0.6452398300170898\n",
      "142 > Best val_loss model saved: -2.6552\n",
      "Saving test scores\n",
      "-2.6555431730606975\n",
      "0.8511764705882353\n",
      "17282 17000\n",
      "epoch:142 -> train_loss=-2.65757,val_loss=-2.65519 - 0.7724123001098633\n",
      "143 > Best val_loss model saved: -2.6557\n",
      "Saving test scores\n",
      "-2.6557832816067863\n",
      "0.8520588235294118\n",
      "17282 17000\n",
      "epoch:143 -> train_loss=-2.65798,val_loss=-2.65565 - 0.6518995761871338\n",
      "Saving test scores\n",
      "-2.655424545792972\n",
      "0.850764705882353\n",
      "17282 17000\n",
      "epoch:144 -> train_loss=-2.65826,val_loss=-2.65512 - 0.7741067409515381\n",
      "Saving test scores\n",
      "-2.6548731397179997\n",
      "0.843764705882353\n",
      "17282 17000\n",
      "epoch:145 -> train_loss=-2.65767,val_loss=-2.65462 - 0.6469366550445557\n",
      "Saving test scores\n",
      "-2.6543978102066936\n",
      "0.8422941176470589\n",
      "17282 17000\n",
      "epoch:146 -> train_loss=-2.65758,val_loss=-2.6541 - 0.6409220695495605\n",
      "Saving test scores\n",
      "-2.654810309410095\n",
      "0.8453529411764706\n",
      "17282 17000\n",
      "epoch:147 -> train_loss=-2.65766,val_loss=-2.65464 - 0.7589206695556641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving test scores\n",
      "-2.654274246271919\n",
      "0.8418823529411765\n",
      "17282 17000\n",
      "epoch:148 -> train_loss=-2.65769,val_loss=-2.6542 - 0.6491000652313232\n",
      "Saving test scores\n",
      "-2.6533141837400547\n",
      "0.8402352941176471\n",
      "17282 17000\n",
      "epoch:149 -> train_loss=-2.65773,val_loss=-2.6534 - 0.7698721885681152\n",
      "Finished Training\n",
      "Saving test scores\n",
      "-2.6557832816067863\n",
      "0.8520588235294118\n",
      "17282 17000\n",
      "class avg accuracy 78.71974440053688\n",
      "MSE 0.13119819830167095\n",
      "MAE 0.2715623887111159\n",
      "MRE 0.07630115136340934\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %load main.py\n",
    "from utilities import make_log_folder\n",
    "import shutil\n",
    "from testing_functions import run_some_linear_models,run_nn\n",
    "from IPython import get_ipython\n",
    "import time\n",
    "if __name__ == '__main__':\n",
    "    make_log_folder(log_folder_name=\"logs\")\n",
    "    # run_linear_model_with_under_and_over_sampling(file_name=\"socfb-American75\", force_recreate_datasets=False, write_train_val_test=False)\n",
    "    ## result: linear_regression: Accuracy=52.527%, MSE=0.492, MAE=0.558, MRE=0.2\n",
    "    #run_some_linear_models(file_name=\"fb-pages-food\", force_recreate_datasets=True, write_train_val_test=False)\n",
    "    ## result: linear_regression: Accuracy = 69.712 %, MSE = 0.248, MAE = 0.393, MRE = 0.16\n",
    "    \n",
    "    \n",
    "    #graph = [\"as-internet\",\"socfb-American75\",\"socfb-OR\"]\n",
    "    \"\"\"\n",
    "    run portion = 0.2, method = top_degrees, you can get many possiable results(stored in log files) like below, \n",
    "    which doesn't make sense', since all the data is the same for every run, \n",
    "    then all the accuracy and mae should also be same\n",
    "    {'acc': '6.6667', 'mse': '4.1998', 'mae': '1.6779', 'mre': '0.376'}\n",
    "    {'acc': '68.7586', 'mse': '0.1565', 'mae': '0.2963', 'mre': '0.0821'}\n",
    "    {'acc': '60.1372', 'mse': '0.2511', 'mae': '0.3724', 'mre': '0.0997'}\n",
    "    And other tests also have the same probelms when we deal with small graphs like inf-euroroad\n",
    "    method can be either random or top_degrees\n",
    "    \"\"\"\n",
    "    \n",
    "    graph_name = 'fb-pages-food'\n",
    "    run_nn(file_name=graph_name, force_recreate_datasets=True, portion = 0.2,\n",
    "           method = \"top_degrees\",write_train_val_test=False)\n",
    "    time.sleep(10)\n",
    "    cmd = \"rm -r \" + \"../output/nn_return/{}\".format(graph_name)\n",
    "    os.system(cmd)\n",
    "    \n",
    "    \n",
    "  \n",
    "    \"\"\"\n",
    "    graph = ['fb-pages-food']\n",
    "    portions = [0.2,0.3,0.4,0.5,0.6,0.7,0.8]\n",
    "    methods = ['random','top_degree']\n",
    "    Acc_r = []\n",
    "    Mae_r = []\n",
    "    Acc_t = []\n",
    "    Mae_t = []\n",
    "    \n",
    "    for i in range(8):\n",
    "        for graph_name in graph:\n",
    "            for method in methods:\n",
    "                tem_acc_r = []\n",
    "                tem_mae_r = []\n",
    "                tem_acc_t = []\n",
    "                tem_mae_t = []\n",
    "                for portion in portions:\n",
    "                    if(method == 'random'):\n",
    "                         get_ipython().magic('reset -sf')\n",
    "                         run_nn(Acc = tem_acc_r,Mae = tem_mae_r,file_name=graph_name, force_recreate_datasets=True, portion = portion,method = method,write_train_val_test=False)\n",
    "                         shutil.rmtree(\"../output/nn_return/{}\".format(graph_name))# just remove return\n",
    "                         time.sleep(50)\n",
    "                    else:\n",
    "                         get_ipython().magic('reset -sf')\n",
    "                         run_nn(Acc = tem_acc_t,Mae = tem_mae_t,file_name=graph_name, force_recreate_datasets=True, portion = portion,method = method,write_train_val_test=False)\n",
    "                         shutil.rmtree(\"../output/nn_return/{}\".format(graph_name))# just remove return\n",
    "                         time.sleep(50)\n",
    "                if(method == 'random'):\n",
    "                    Acc_r.append(tem_acc_r)\n",
    "                    Mae_r.append(tem_mae_r)\n",
    "                else:\n",
    "                    Acc_t.append(tem_acc_t)\n",
    "                    Mae_t.append(tem_mae_t)\n",
    "  \n",
    "                    \n",
    "    ## result: nn: Accuracy 41.268%,  MSE = 0.225, MAE = 0.37, MRE = 0.14\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f63ded3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "226d2063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ced7387",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
